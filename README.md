# Evolutionary Prompt Optimization for LLMs

**Research Internship Project | TUM - Computer Aided Medical Procedures (CAMP)**

This repository contains the code and results from my summer research internship at the Technical University of Munich (TUM). The project focuses on automating prompt engineering using **Genetic Algorithms (GA)** to optimize system instructions for Large Language Models (LLMs).

## üìÑ Project Overview

Manual prompt engineering is time-consuming and often unintuitive. This project proposes an evolutionary approach where a "population" of prompts evolves over generations to maximize performance on reasoning benchmarks (MMLU-Pro and BBH).

### The Pipeline
The system treats prompt instructions as a genome. A binary vector represents the activation of specific prompt attributes (e.g., "Chain of Thought", "Persona: Expert", "Conciseness").

![Methodology Diagram](results/pipeline_diagram.png)
*Figure 1: The Evolutionary Pipeline. A binary genome is mapped to a text prompt, generated by the LLM, evaluated against ground truth, and evolved via crossover and mutation.*

## üõ†Ô∏è Methodology

The core logic utilizes the **DEAP** library for evolutionary computation and **vLLM** for high-throughput inference.

*   **Gene Pool:** defined in `src/attributes_llm.py`, containing 40+ distinct prompt strategies (Role-playing, CoT, Formatting constraints, Emotional stimuli).
*   **Optimization:** The GA (in `src/ga_vllm_opt.py`) evolves the population to maximize accuracy on a training subset of MMLU/BBH.
*   **Models Evaluated:** Qwen3-8B, Phi-4-mini, Gemma-3, and Llama-3.1.

## üìä Results

### 1. Performance Gains
The evolutionary approach consistently outperformed baseline direct prompting and standard Zero-Shot Chain-of-Thought (CoT).

![Benchmark Results](results/benchmark_table.png)
*Table 1: Comparison of Direct prompting, Zero-Shot CoT, and the GA-Optimized prompt. The GA method achieved the highest accuracy across most models.*

**Key Findings:**
*   **Qwen3-8B** saw a significant boost, reaching **77.88%** on BBH using the optimized prompt.
*   The GA successfully identified synergistic combinations of instructions (e.g., combining "Expert Persona" with "Step-back Prompting") that humans might overlook.

### 2. Cross-Model Transferability
We tested whether a prompt optimized for one model (e.g., Qwen) works well on another (e.g., Llama).

![Transferability Heatmap](results/transfer_heatmap.png)
*Figure 2: Cross-Model Transferability Matrix. Rows represent the model used to optimize the prompt; Columns represent the model evaluated.*

*   **Green Cells:** High transferability. Prompts optimized on Qwen3-8B generalized exceptionally well to Phi-4 and Gemma-3.
*   **Observation:** Larger or more capable models tend to generate more robust prompts that work universally.

## üíª Usage

### Requirements
*   Python 3.10+
*   CUDA-compatible GPU (High VRAM recommended for vLLM)

```bash
pip install -r requirements.txt