# Evolutionary Prompt Optimization for LLMs

**Research Internship Project | TUM - Computer Aided Medical Procedures (CAMP)**

This repository contains the code and results from my summer research internship at the Technical University of Munich (TUM). The project focuses on automating prompt engineering using **Genetic Algorithms (GA)** to optimize system instructions for Large Language Models (LLMs).

## üìÑ Project Overview

Manual prompt engineering is time-consuming and often unintuitive. This project proposes an evolutionary approach where a "population" of prompts evolves over generations to maximize performance on reasoning benchmarks (MMLU-Pro and BBH).

### The Pipeline
The system treats prompt instructions as a genome. A binary vector represents the activation of specific prompt attributes (e.g., "Chain of Thought", "Persona: Expert", "Conciseness").

![Methodology Diagram](diagram/pipeline.png)
*Figure 1: The Evolutionary Pipeline. A binary genome is mapped to a text prompt, generated by the LLM, evaluated against ground truth, and evolved via crossover and mutation.*

## üõ†Ô∏è Methodology

The core logic utilizes the **DEAP** library for evolutionary computation and **vLLM** for high-throughput inference.

*   **Gene Pool:** defined in `src/attributes_llm.py`, containing 40+ distinct prompt strategies (Role-playing, CoT, Formatting constraints, Emotional stimuli).
*   **Optimization:** The GA (in `src/ga_vllm_opt.py`) evolves the population to maximize accuracy on a training subset of MMLU/BBH.
*   **Models Evaluated:** Qwen3-8B, Phi-4-mini, Gemma-3, and Llama-3.1.

## üìä Results & Analysis

We evaluated the evolutionary framework across multiple dimensions, including strict formatting constraints, token budgets, and cross-domain generalization.

### 1. Optimization under Constraints (JSON & Token Budget)
A major challenge for LLMs is maintaining reasoning quality while adhering to strict output formats (e.g., JSON) or low token budgets (e.g., 512 tokens). We compared the GA-Optimized prompts against Direct Prompting, Zero-Shot CoT, and a manual "Concise CoT" baseline.

![JSON 512 Results](results/json-512.png)
*Table 1: Performance with strict JSON output enforcement and a 512-token limit.*

**Key Findings:**
*   **Superior Constraint Adherence:** The GA evolved prompts that successfully forced models to output valid JSON while performing complex reasoning, significantly outperforming standard Zero-Shot CoT, which often breaks formatting rules.
*   **Efficiency:** Even with a reduced token budget (512 vs. the standard 2048), the GA-optimized prompts maintained high accuracy, making them suitable for production environments where latency and cost are factors.

### 2. Cross-Dataset Generalization
To test if the GA simply "overfits" to a specific dataset, we took prompts optimized on **MMLU-Pro** and tested them on **BBH** (and vice versa).

![Cross Dataset Results](results/cross-dataset.png)
*Table 2: Generalization performance. "Difference" indicates the performance drop when applying a prompt optimized on one dataset to a completely different one.*

**Observation:**
*   The system demonstrates **task-agnostic generalization**. For example, the prompt evolved for Qwen3-8B on MMLU-Pro lost only **0.02%** accuracy when applied to BBH.
*   This suggests the GA is discovering fundamental reasoning instructions (e.g., "Step-back prompting" combined with "Persona adoption") that apply universally to logic tasks, rather than dataset-specific tricks.

### 3. Cross-Model Transferability
We analyzed the transferability of optimized prompts across different model architectures. The heatmap below displays the accuracy when a prompt optimized on a *Source Model* (rows) is evaluated on a *Target Model* (columns).

![Cross Model Heatmap](results/cross-model.png)
*Figure 2: Transferability Matrix. Green indicates high performance; Orange indicates lower performance.*

**Insights:**
*   **Strong-to-Weak Transfer:** Prompts optimized on capable models (like **Qwen3-8B**) generalize exceptionally well to smaller or different models (Phi-4-mini, Gemma-3).
*   **Model-Specific Prompts:** Llama-3.1 generated prompts that were highly effective for itself but transferred poorly to others, indicating it found model-specific "hacks" rather than universal reasoning patterns.

## üíª Usage

### Requirements
*   Python 3.10+
*   CUDA-compatible GPU (High VRAM recommended for vLLM)

```bash
pip install -r requirements.txt

## üìÇ Repository Structure

*   `ga_vllm_opt.py`: The main script running the Genetic Algorithm using DEAP and vLLM.
*   `attributes_llm.py`: Contains the "Gene Pool"‚Äîthe dictionary of prompt instructions, prefixes, and styles used to construct the genome.
*   `eval_cot.py`: Evaluation pipeline for benchmarking models against MMLU-Pro and BBH using various prompting strategies (Zero-Shot, Few-Shot, CoT).
*   `utils.py`: Helper functions for GPU detection, memory management, and parsing JSON answers from LLM outputs.
*   `results/`: Contains visualizations of the experiments, including the pipeline diagram, transferability heatmaps, and performance tables.

## Acknowledgments

This research was conducted during my internship at the **Computer Aided Medical Procedures (CAMP)** Chair at **Technical University of Munich (TUM)**. I would like to thank my supervisors for their mentorship and support throughout this project.